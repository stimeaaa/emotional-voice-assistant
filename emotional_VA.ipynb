{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee435eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 21:20:18.359272: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-03 21:20:18.407101: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-03 21:20:18.407133: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-03 21:20:18.407158: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-03 21:20:18.414377: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "import sys \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gradio as gr\n",
    "import torch\n",
    "import json\n",
    "import IPython.display as ipd\n",
    "import faiss\n",
    "import soundfile as sf\n",
    "import fitz  \n",
    "import re\n",
    "import gradio as gr\n",
    "import time\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from IPython.display import Audio\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from pathlib import Path\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from scipy.io.wavfile import write\n",
    "from transformers import pipeline\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22836584",
   "metadata": {},
   "source": [
    "## estimate gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8015d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gender(audio_path):\n",
    "    \n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "    f0 = librosa.pyin(y, fmin=75, fmax=300, sr=sr, \n",
    "                     frame_length=2048, win_length=1024, hop_length=256)\n",
    "    f0_values = f0[0]\n",
    "    f0_values = f0_values[~np.isnan(f0_values)]\n",
    "    \n",
    "    if len(f0_values) == 0:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    f0_median = np.median(f0_values)\n",
    "    print(f0_median)\n",
    "   \n",
    "    if f0_median <= 130:\n",
    "        return \"Male\"\n",
    "    elif f0_median >= 165:\n",
    "        return \"Female\"\n",
    "    else:\n",
    "       \n",
    "        f0_mean = np.mean(f0_values)\n",
    "        f0_std = np.std(f0_values)\n",
    "        \n",
    "        if f0_median < 145 and f0_std < 25:\n",
    "            return \"Male\"\n",
    "        else:\n",
    "            return \"Female\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a6f7c",
   "metadata": {},
   "source": [
    "## LSTM emotion rec.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3b67b",
   "metadata": {},
   "source": [
    "### Előkészítés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3116db69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is Loaded\n",
      "Dataset is Loaded\n",
      "Dataset is Loaded\n",
      "Dataset is Loaded\n",
      "Dataset is Loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/simon/ESD/0019/Surprise/0019_001592.wav</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/simon/ESD/0019/Surprise/0019_001572.wav</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/simon/ESD/0019/Surprise/0019_001613.wav</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/simon/ESD/0019/Surprise/0019_001741.wav</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/simon/ESD/0019/Surprise/0019_001512.wav</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          speech     label\n",
       "0  /home/simon/ESD/0019/Surprise/0019_001592.wav  surprise\n",
       "1  /home/simon/ESD/0019/Surprise/0019_001572.wav  surprise\n",
       "2  /home/simon/ESD/0019/Surprise/0019_001613.wav  surprise\n",
       "3  /home/simon/ESD/0019/Surprise/0019_001741.wav  surprise\n",
       "4  /home/simon/ESD/0019/Surprise/0019_001512.wav  surprise"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_paths = []\n",
    "val_labels = []\n",
    "for i in range(19,21):\n",
    "    for dirname, _, filenames in os.walk(f\"/home/simon/ESD/00{i}/Surprise\"):\n",
    "        for filename in filenames:\n",
    "            val_paths.append(os.path.join(dirname,filename))\n",
    "            label=\"surprise\"\n",
    "            val_labels.append(label.lower())\n",
    "\n",
    "print(\"Dataset is Loaded\")\n",
    "for i in range(19,21):\n",
    "    for dirname, _, filenames in os.walk(f\"/home/simon/ESD/00{i}/Sad\"):\n",
    "        for filename in filenames:\n",
    "            val_paths.append(os.path.join(dirname,filename))\n",
    "            label=\"sad\"\n",
    "            val_labels.append(label.lower())\n",
    "\n",
    "print(\"Dataset is Loaded\")\n",
    "for i in range(19,21):\n",
    "    for dirname, _, filenames in os.walk(f\"/home/simon/ESD/00{i}/Happy\"):\n",
    "        for filename in filenames:\n",
    "            val_paths.append(os.path.join(dirname,filename))\n",
    "            label=\"happy\"\n",
    "            val_labels.append(label.lower())\n",
    "\n",
    "print(\"Dataset is Loaded\")\n",
    "for i in range(19,21):\n",
    "    for dirname, _, filenames in os.walk(f\"/home/simon/ESD/00{i}/Angry\"):\n",
    "        for filename in filenames:\n",
    "            val_paths.append(os.path.join(dirname,filename))\n",
    "            label=\"angry\"\n",
    "            val_labels.append(label.lower())\n",
    "\n",
    "print(\"Dataset is Loaded\")\n",
    "for i in range(19,21):\n",
    "    for dirname, _, filenames in os.walk(f\"/home/simon/ESD/00{i}/Neutral\"):\n",
    "        for filename in filenames:\n",
    "            val_paths.append(os.path.join(dirname,filename))\n",
    "            label=\"neutral\"\n",
    "            val_labels.append(label.lower())\n",
    "\n",
    "print(\"Dataset is Loaded\")\n",
    "df_val= pd.DataFrame()\n",
    "df_val['speech']=val_paths\n",
    "df_val['label']=val_labels\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b342f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(filename):\n",
    "    y,sr=librosa.load(filename, duration=3, offset=0.5)\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff0fdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.5702924e+02,  9.6109161e+01,  1.8062576e+00,  2.2582365e+01,\n",
       "       -3.1091631e+01, -1.7898973e+01, -1.5407653e+01, -4.4629386e-01,\n",
       "       -2.4464369e+01, -7.5679815e-01, -7.0336785e+00, -1.7772552e+01,\n",
       "        1.0111096e+00, -1.0597541e+01, -1.2515818e+00, -6.2318811e+00,\n",
       "        2.3008943e+00, -1.0849600e+00, -3.3185184e+00,  8.5988939e-01,\n",
       "       -1.7053005e+00, -4.2913432e+00, -3.9740238e+00, -1.0241350e+00,\n",
       "       -4.7427807e+00, -3.0093232e-01, -1.0000892e+00,  4.3346378e-01,\n",
       "       -5.7896429e-01,  9.5399719e-01, -4.0370992e-01, -2.1399801e+00,\n",
       "       -1.8853335e+00,  9.6497232e-01,  2.8253663e+00,  1.4093702e+00,\n",
       "        5.7589135e+00,  3.7085714e+00,  5.6110139e+00,  4.2233911e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_mfcc(df_val['speech'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3359c98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 5)\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_mfcc_val = df_val['speech'].apply(lambda x: extract_mfcc(x))\n",
    "X_mfcc_val\n",
    "X_val_esd = [x for x in X_mfcc_val]\n",
    "X_val_esd = np.array(X_val_esd)\n",
    "X_val_esd.shape\n",
    "X_val_esd = np.expand_dims(X_val_esd,-1)\n",
    "X_val_esd.shape\n",
    "X_val_esd=X_val_esd.astype('float32')\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_val_esd = enc.fit_transform(df_val[['label']]).astype('float32')\n",
    "\n",
    "print(y_val_esd.shape)  \n",
    "print(np.unique(y_val_esd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef1b271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 14619\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "labels = []\n",
    "\n",
    "new_dataset_path = \"/root/.cache/kagglehub/datasets/dejolilandry/asvpesdspeech-nonspeech-emotional-utterances/versions/19/ASVP-ESD-Update/Audio\"\n",
    "\n",
    "allowed_emotions = {\n",
    "    \"02\": \"neutral\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"08\": \"surprise\"\n",
    "}\n",
    "\n",
    "for root, dirs, files in os.walk(new_dataset_path):\n",
    "    for file in files:\n",
    "        if not file.endswith(\".wav\"):\n",
    "            continue\n",
    "        \n",
    "        parts = file.split(\"-\")\n",
    "        if len(parts) < 9:\n",
    "            continue \n",
    "        \n",
    "        emotion_code = parts[2]\n",
    "        language_code = parts[8]\n",
    "        \n",
    "        if emotion_code in allowed_emotions and language_code==\"02\":\n",
    "            paths.append(os.path.join(root, file))\n",
    "            labels.append(allowed_emotions[emotion_code])\n",
    "\n",
    "\n",
    "for i in range(11,18):\n",
    "    for dirname, _, filenames in os.walk(f\"/home/simon/ESD/00{i}/Happy\"):\n",
    "        for filename in filenames:\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            labels.append(\"happy\")\n",
    "\n",
    "for i in range(11,18):\n",
    "    for dirname, _, filenames in os.walk(f\"/home/simon/ESD/00{i}/Angry\"):\n",
    "        for filename in filenames:\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            labels.append(\"angry\")\n",
    "\n",
    "for i in range(11,18):\n",
    "    for dirname, _, filenames in os.walk(f\"/home/simon/ESD/00{i}/Neutral\"):\n",
    "        for filename in filenames:\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            labels.append(\"neutral\")\n",
    "\n",
    "for i in range(11,18):\n",
    "    for dirname, _, filenames in os.walk(f\"/home/simon/ESD/00{i}/Surprise\"):\n",
    "        for filename in filenames:\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            labels.append(\"surprise\")\n",
    "\n",
    "for i in range(11,18):\n",
    "    for dirname, _, filenames in os.walk(f\"/home/simon/ESD/00{i}/Sad\"):\n",
    "        for filename in filenames:\n",
    "            paths.append(os.path.join(dirname, filename))\n",
    "            labels.append(\"sad\")\n",
    "\n",
    "print(\"Total samples:\", len(paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "059badb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/root/.cache/kagglehub/datasets/dejolilandry/a...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/root/.cache/kagglehub/datasets/dejolilandry/a...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/root/.cache/kagglehub/datasets/dejolilandry/a...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/root/.cache/kagglehub/datasets/dejolilandry/a...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/root/.cache/kagglehub/datasets/dejolilandry/a...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              speech     label\n",
       "0  /root/.cache/kagglehub/datasets/dejolilandry/a...     angry\n",
       "1  /root/.cache/kagglehub/datasets/dejolilandry/a...  surprise\n",
       "2  /root/.cache/kagglehub/datasets/dejolilandry/a...  surprise\n",
       "3  /root/.cache/kagglehub/datasets/dejolilandry/a...  surprise\n",
       "4  /root/.cache/kagglehub/datasets/dejolilandry/a...  surprise"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.DataFrame()\n",
    "df['speech']=paths\n",
    "df['label']=labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dd57fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-600.97076  ,  100.06333  ,  -42.26156  ,   35.409367 ,\n",
       "         16.721735 ,   15.365932 ,  -14.453442 ,    3.399201 ,\n",
       "         -1.9597461,  -25.75174  ,   -6.6531873,   12.56852  ,\n",
       "          7.4000473,    6.1022797,   -7.207781 ,  -19.00995  ,\n",
       "         -9.826457 ,    3.327453 ,    1.8901261,   -8.150221 ,\n",
       "         -1.2986673,   -3.23105  ,    5.3001704,    2.9816468,\n",
       "         -3.1129587,   -4.3711624,  -14.622587 ,   -1.1081332,\n",
       "         -2.4216607,   -8.203386 ,   -5.3483057,   -7.2141714,\n",
       "         -2.3185892,    5.3036013,   10.792786 ,    8.713029 ,\n",
       "          3.4217293,   -2.552914 ,   -5.5329266,   -2.4955325],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_mfcc(df['speech'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4ced68cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad path num: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "bad_paths = [p for p in df['speech'] if not os.path.isfile(p)]\n",
    "print(\"Bad path num:\", len(bad_paths))\n",
    "print(bad_paths[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87c972b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_path = \"/home/simon/ESD/0013/Sad/Sad\"\n",
    "df = df[df['speech'] != bad_path]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23db30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mfcc = df['speech'].apply(lambda x: extract_mfcc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95230166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14618, 40)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [x for x in X_mfcc]\n",
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5593e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14618, 5)\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y = enc.fit_transform(df[['label']]).astype('float32')\n",
    "\n",
    "print(y.shape)\n",
    "print(np.unique(y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f027f",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d6a4f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 21:24:57.495551: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 123)               61500     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                7936      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71681 (280.00 KB)\n",
      "Trainable params: 71681 (280.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM , Dropout\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(123,return_sequences=False, input_shape=(40,1)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e6d3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot = to_categorical(y, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e4b787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 [==============================] - 6s 110ms/step - loss: 1.5628 - accuracy: 0.2831 - val_loss: 1.5330 - val_accuracy: 0.2860\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 3s 86ms/step - loss: 1.4569 - accuracy: 0.3578 - val_loss: 1.5471 - val_accuracy: 0.3000\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 3s 87ms/step - loss: 1.3764 - accuracy: 0.3981 - val_loss: 1.4581 - val_accuracy: 0.3477\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 1.2954 - accuracy: 0.4360 - val_loss: 1.4009 - val_accuracy: 0.3814\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 1.2571 - accuracy: 0.4533 - val_loss: 1.3899 - val_accuracy: 0.3877\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 1.2298 - accuracy: 0.4703 - val_loss: 1.5459 - val_accuracy: 0.3286\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 1.1977 - accuracy: 0.4843 - val_loss: 1.5992 - val_accuracy: 0.3446\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 1.1921 - accuracy: 0.4904 - val_loss: 1.3795 - val_accuracy: 0.3757\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 1.1594 - accuracy: 0.5103 - val_loss: 1.5498 - val_accuracy: 0.3389\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 2s 84ms/step - loss: 1.1280 - accuracy: 0.5257 - val_loss: 1.4847 - val_accuracy: 0.3663\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 1.1053 - accuracy: 0.5374 - val_loss: 1.4786 - val_accuracy: 0.3983\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 1.0845 - accuracy: 0.5460 - val_loss: 1.4499 - val_accuracy: 0.3814\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 1.0548 - accuracy: 0.5592 - val_loss: 1.5302 - val_accuracy: 0.3663\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 2s 80ms/step - loss: 1.0288 - accuracy: 0.5767 - val_loss: 1.5405 - val_accuracy: 0.3820\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 1.0215 - accuracy: 0.5769 - val_loss: 1.4924 - val_accuracy: 0.3914\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 2s 79ms/step - loss: 0.9970 - accuracy: 0.5899 - val_loss: 1.5087 - val_accuracy: 0.3814\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 0.9731 - accuracy: 0.6036 - val_loss: 1.5865 - val_accuracy: 0.4083\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 2s 80ms/step - loss: 0.9485 - accuracy: 0.6209 - val_loss: 1.5152 - val_accuracy: 0.3971\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 2s 79ms/step - loss: 0.9107 - accuracy: 0.6390 - val_loss: 1.6372 - val_accuracy: 0.3823\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 2s 79ms/step - loss: 0.8951 - accuracy: 0.6445 - val_loss: 1.5873 - val_accuracy: 0.3746\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.8713 - accuracy: 0.6636 - val_loss: 1.6670 - val_accuracy: 0.3669\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 2s 79ms/step - loss: 0.8494 - accuracy: 0.6716 - val_loss: 1.6071 - val_accuracy: 0.3717\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 2s 78ms/step - loss: 0.8304 - accuracy: 0.6787 - val_loss: 1.6983 - val_accuracy: 0.3674\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.8310 - accuracy: 0.6786 - val_loss: 1.5283 - val_accuracy: 0.4197\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 0.7946 - accuracy: 0.6986 - val_loss: 1.6896 - val_accuracy: 0.3571\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 2s 79ms/step - loss: 0.7802 - accuracy: 0.7002 - val_loss: 1.7505 - val_accuracy: 0.3729\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.7729 - accuracy: 0.7044 - val_loss: 1.6886 - val_accuracy: 0.3737\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 2s 79ms/step - loss: 0.7476 - accuracy: 0.7181 - val_loss: 1.7665 - val_accuracy: 0.3614\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 2s 84ms/step - loss: 0.7273 - accuracy: 0.7199 - val_loss: 1.6288 - val_accuracy: 0.4117\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.7250 - accuracy: 0.7244 - val_loss: 1.7314 - val_accuracy: 0.3614\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.7162 - accuracy: 0.7272 - val_loss: 1.7195 - val_accuracy: 0.3897\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 2s 80ms/step - loss: 0.6924 - accuracy: 0.7408 - val_loss: 1.8687 - val_accuracy: 0.3394\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 2s 84ms/step - loss: 0.6897 - accuracy: 0.7372 - val_loss: 1.7278 - val_accuracy: 0.3597\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.6798 - accuracy: 0.7455 - val_loss: 1.8644 - val_accuracy: 0.3403\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.6634 - accuracy: 0.7501 - val_loss: 1.7531 - val_accuracy: 0.3717\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.6606 - accuracy: 0.7485 - val_loss: 1.8260 - val_accuracy: 0.3709\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 0.6443 - accuracy: 0.7545 - val_loss: 1.9569 - val_accuracy: 0.3386\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.6358 - accuracy: 0.7596 - val_loss: 1.9361 - val_accuracy: 0.3423\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 2s 78ms/step - loss: 0.6300 - accuracy: 0.7625 - val_loss: 1.8678 - val_accuracy: 0.3471\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 2s 78ms/step - loss: 0.6193 - accuracy: 0.7689 - val_loss: 1.8507 - val_accuracy: 0.3626\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 2s 86ms/step - loss: 0.6147 - accuracy: 0.7684 - val_loss: 1.8794 - val_accuracy: 0.3609\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.5834 - accuracy: 0.7833 - val_loss: 1.9342 - val_accuracy: 0.3746\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 2s 85ms/step - loss: 0.5853 - accuracy: 0.7813 - val_loss: 1.9043 - val_accuracy: 0.3394\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.5864 - accuracy: 0.7786 - val_loss: 1.9668 - val_accuracy: 0.3471\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 2s 84ms/step - loss: 0.5617 - accuracy: 0.7911 - val_loss: 2.0193 - val_accuracy: 0.3506\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 2s 85ms/step - loss: 0.5603 - accuracy: 0.7903 - val_loss: 2.0555 - val_accuracy: 0.3554\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.5427 - accuracy: 0.7955 - val_loss: 2.0243 - val_accuracy: 0.3343\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.5472 - accuracy: 0.7943 - val_loss: 2.1496 - val_accuracy: 0.3391\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 0.5316 - accuracy: 0.7987 - val_loss: 2.0990 - val_accuracy: 0.3477\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.5373 - accuracy: 0.7975 - val_loss: 2.0797 - val_accuracy: 0.3480\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.5253 - accuracy: 0.8022 - val_loss: 2.0658 - val_accuracy: 0.3269\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.5151 - accuracy: 0.8116 - val_loss: 2.1638 - val_accuracy: 0.3283\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.5206 - accuracy: 0.8070 - val_loss: 2.1805 - val_accuracy: 0.3569\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 2s 85ms/step - loss: 0.4989 - accuracy: 0.8125 - val_loss: 2.2112 - val_accuracy: 0.3491\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 2s 80ms/step - loss: 0.4936 - accuracy: 0.8134 - val_loss: 2.2796 - val_accuracy: 0.3229\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.4850 - accuracy: 0.8189 - val_loss: 2.2716 - val_accuracy: 0.3514\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 2s 84ms/step - loss: 0.4695 - accuracy: 0.8224 - val_loss: 2.2193 - val_accuracy: 0.3491\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 2s 80ms/step - loss: 0.4593 - accuracy: 0.8302 - val_loss: 2.3046 - val_accuracy: 0.3380\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 2s 80ms/step - loss: 0.4596 - accuracy: 0.8291 - val_loss: 2.2524 - val_accuracy: 0.3400\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 3s 88ms/step - loss: 0.4550 - accuracy: 0.8312 - val_loss: 2.2230 - val_accuracy: 0.3554\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.4427 - accuracy: 0.8333 - val_loss: 2.2956 - val_accuracy: 0.3606\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 2s 84ms/step - loss: 0.4278 - accuracy: 0.8394 - val_loss: 2.2861 - val_accuracy: 0.3591\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.4382 - accuracy: 0.8351 - val_loss: 2.5006 - val_accuracy: 0.3329\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 2s 85ms/step - loss: 0.4118 - accuracy: 0.8463 - val_loss: 2.4968 - val_accuracy: 0.3389\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 2s 80ms/step - loss: 0.4372 - accuracy: 0.8360 - val_loss: 2.2326 - val_accuracy: 0.3471\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 2s 84ms/step - loss: 0.4105 - accuracy: 0.8481 - val_loss: 2.4456 - val_accuracy: 0.3520\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 2s 84ms/step - loss: 0.4074 - accuracy: 0.8494 - val_loss: 2.4457 - val_accuracy: 0.3769\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 2s 86ms/step - loss: 0.3923 - accuracy: 0.8531 - val_loss: 2.5282 - val_accuracy: 0.3437\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.3850 - accuracy: 0.8578 - val_loss: 2.5318 - val_accuracy: 0.3463\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 2s 78ms/step - loss: 0.3914 - accuracy: 0.8531 - val_loss: 2.5142 - val_accuracy: 0.3480\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 3s 87ms/step - loss: 0.3677 - accuracy: 0.8658 - val_loss: 2.4964 - val_accuracy: 0.3486\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.3831 - accuracy: 0.8552 - val_loss: 2.5931 - val_accuracy: 0.3349\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 2s 78ms/step - loss: 0.3731 - accuracy: 0.8639 - val_loss: 2.6145 - val_accuracy: 0.3377\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 3s 90ms/step - loss: 0.3609 - accuracy: 0.8652 - val_loss: 2.7981 - val_accuracy: 0.3660\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 0.3490 - accuracy: 0.8692 - val_loss: 2.4927 - val_accuracy: 0.3651\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.3337 - accuracy: 0.8764 - val_loss: 2.7332 - val_accuracy: 0.3451\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.3617 - accuracy: 0.8659 - val_loss: 2.6394 - val_accuracy: 0.3366\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 3s 88ms/step - loss: 0.3546 - accuracy: 0.8687 - val_loss: 2.6415 - val_accuracy: 0.3391\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 2s 85ms/step - loss: 0.3343 - accuracy: 0.8772 - val_loss: 2.7062 - val_accuracy: 0.3480\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.3166 - accuracy: 0.8828 - val_loss: 2.8413 - val_accuracy: 0.3311\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 2s 84ms/step - loss: 0.3116 - accuracy: 0.8859 - val_loss: 2.8409 - val_accuracy: 0.3534\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 0.3214 - accuracy: 0.8813 - val_loss: 2.6660 - val_accuracy: 0.3651\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.3141 - accuracy: 0.8849 - val_loss: 2.8866 - val_accuracy: 0.3354\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.2831 - accuracy: 0.8967 - val_loss: 3.0894 - val_accuracy: 0.3243\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.2965 - accuracy: 0.8899 - val_loss: 2.9833 - val_accuracy: 0.3360\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 2s 83ms/step - loss: 0.2937 - accuracy: 0.8927 - val_loss: 3.0711 - val_accuracy: 0.3274\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 0.2714 - accuracy: 0.9016 - val_loss: 3.1339 - val_accuracy: 0.3606\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 2s 84ms/step - loss: 0.2857 - accuracy: 0.8939 - val_loss: 3.1174 - val_accuracy: 0.3294\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 2s 84ms/step - loss: 0.2964 - accuracy: 0.8908 - val_loss: 3.0464 - val_accuracy: 0.3363\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 0.2725 - accuracy: 0.9005 - val_loss: 3.2531 - val_accuracy: 0.3380\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 0.2669 - accuracy: 0.9021 - val_loss: 3.1779 - val_accuracy: 0.3477\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 2s 78ms/step - loss: 0.2697 - accuracy: 0.9023 - val_loss: 3.1226 - val_accuracy: 0.3546\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 2s 79ms/step - loss: 0.2449 - accuracy: 0.9120 - val_loss: 3.3001 - val_accuracy: 0.3386\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 2s 77ms/step - loss: 0.2324 - accuracy: 0.9172 - val_loss: 3.4683 - val_accuracy: 0.3374\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 2s 77ms/step - loss: 0.2313 - accuracy: 0.9123 - val_loss: 3.6404 - val_accuracy: 0.3189\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 2s 80ms/step - loss: 0.2401 - accuracy: 0.9124 - val_loss: 3.5178 - val_accuracy: 0.3317\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 2s 79ms/step - loss: 0.2265 - accuracy: 0.9180 - val_loss: 3.6540 - val_accuracy: 0.3217\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 0.2317 - accuracy: 0.9151 - val_loss: 3.6326 - val_accuracy: 0.3337\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 0.2237 - accuracy: 0.9175 - val_loss: 3.5884 - val_accuracy: 0.3557\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 2s 80ms/step - loss: 0.2059 - accuracy: 0.9267 - val_loss: 3.9132 - val_accuracy: 0.3266\n"
     ]
    }
   ],
   "source": [
    "indices_train = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices_train)\n",
    "X = X[indices_train]\n",
    "y = y[indices_train]\n",
    "\n",
    "indices_val = np.arange(X_val_esd.shape[0])\n",
    "np.random.shuffle(indices_val)\n",
    "X_val_esd = X_val_esd[indices_val]\n",
    "y_val_esd = y_val_esd[indices_val]\n",
    "\n",
    "history = model.fit(X, y,\n",
    "                    validation_data=(X_val_esd, y_val_esd),\n",
    "                    epochs=100,\n",
    "                    batch_size=512,\n",
    "                    shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee684735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 2s 11ms/step\n",
      "Validation Accuracy: 32.66%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.37      0.35       700\n",
      "           1       0.26      0.30      0.28       700\n",
      "           2       0.35      0.52      0.42       700\n",
      "           3       0.35      0.19      0.24       700\n",
      "           4       0.36      0.26      0.30       700\n",
      "\n",
      "    accuracy                           0.33      3500\n",
      "   macro avg       0.33      0.33      0.32      3500\n",
      "weighted avg       0.33      0.33      0.32      3500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_esd = model.predict(X_val_esd) \n",
    "pred_classes_esd = np.argmax(y_pred_esd, axis=1)\n",
    "true_classes_esd = np.argmax(y_val_esd, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(true_classes_esd, pred_classes_esd)\n",
    "print(f\"Validation Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "print(classification_report(true_classes_esd, pred_classes_esd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd3b86",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e74a9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_onehot(pred_class, encoder):\n",
    "    onehot = np.zeros((1, len(encoder.categories_[0])))  \n",
    "    onehot[0, pred_class] = 1\n",
    "    return encoder.inverse_transform(onehot)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "116d7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(audio_path, model, encoder, sr=22050, n_mfcc=40):\n",
    "\n",
    "    y, sr = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc_mean = np.mean(mfcc.T, axis=0)\n",
    "\n",
    "    mfcc_input = mfcc_mean.reshape(1, n_mfcc, 1) \n",
    "\n",
    "    pred = model.predict(mfcc_input)\n",
    "    pred_class = np.argmax(pred, axis=1)[0]\n",
    "\n",
    "    return decode_onehot(pred_class, encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fd16369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "Predicted emotion: surprise\n"
     ]
    }
   ],
   "source": [
    "audio_path=\"/home/simon/ESD/0019/Happy/0019_000707.wav\"\n",
    "predicted_emotion = predict_emotion(audio_path, model, enc)\n",
    "print(\"Predicted emotion:\", predicted_emotion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd8857",
   "metadata": {},
   "source": [
    "## whisper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c168553",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "whisper = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-medium\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    generate_kwargs={\n",
    "        \"language\": \"en\",       \n",
    "        \"task\": \"transcribe\"   \n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def hang_felismeres(hang):\n",
    "    if hang is None:\n",
    "        return None, \"Nem kaptam hangot.\"\n",
    "\n",
    "    samplerate, data = hang  \n",
    "    filepath = \"felvetel.wav\"\n",
    "\n",
    "\n",
    "    write(filepath, samplerate, data.astype(np.int16))\n",
    "\n",
    "    eredmeny = whisper(filepath)\n",
    "    text = eredmeny[\"text\"]\n",
    "\n",
    "    return filepath, text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073f859",
   "metadata": {},
   "source": [
    "## Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec8efcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "005adc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def smart_chunk_text(text, max_chunk_size=800, overlap=120):\n",
    "    chunks = []\n",
    "\n",
    "    title_pattern = re.compile(r\"(^|\\n)([A-Z][A-Za-z0-9 ,:-]{3,})\\n\")\n",
    "    parts = title_pattern.split(text)\n",
    "\n",
    "    paired_sections = []\n",
    "    current_title = \"Unknown section\"\n",
    "    \n",
    "    for i in range(len(parts)):\n",
    "        part = parts[i].strip()\n",
    "        if i % 3 == 2:   \n",
    "            current_title = part\n",
    "        elif i % 3 == 0 and part:\n",
    "            paired_sections.append((current_title, part))\n",
    "    \n",
    "    for title, content in paired_sections:\n",
    "        paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if len(p.strip()) > 0]\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for para in paragraphs:\n",
    "            if len(current_chunk) + len(para) < max_chunk_size:\n",
    "                current_chunk += para + \"\\n\\n\"\n",
    "            else:\n",
    "                chunks.append({\"title\": title, \"content\": current_chunk.strip()})\n",
    "                overlap_text = current_chunk[-overlap:]\n",
    "                current_chunk = overlap_text + para + \"\\n\\n\"\n",
    "\n",
    "        if current_chunk.strip():\n",
    "            chunks.append({\"title\": title, \"content\": current_chunk.strip()})\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c63c4f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedder = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n",
    "\n",
    "def create_embeddings(chunks):\n",
    "    text_data = [ch[\"title\"] + \"\\n\" + ch[\"content\"] for ch in chunks]\n",
    "    \n",
    "    embeddings = embedder.encode(text_data, normalize_embeddings=True)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(np.array(embeddings))\n",
    "    \n",
    "    return index, embeddings, text_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c51aea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, index, text_data, top_k=3):\n",
    "    query_emb = embedder.encode([query], normalize_embeddings=True)\n",
    "    distances, indices = index.search(np.array(query_emb), top_k)\n",
    "    results = [text_data[i] for i in indices[0]]\n",
    "    return \"\\n\\n---\\n\\n\".join(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adae196",
   "metadata": {},
   "source": [
    "## Fastpitch modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "daf1a1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "home_path = !(echo $HOME)\n",
    "home_path = home_path[0]\n",
    "print(home_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64723a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/simon\r\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f0220c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I1203 21:29:08.430548 140008601651008 tokenize_and_classify.py:87] Creating ClassifyFst grammars.\n",
      "[NeMo W 2025-12-03 21:29:33 en_us_arpabet:66] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
      "[NeMo W 2025-12-03 21:29:33 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /ws/HiFiTTS/manifest_train_ngc.json\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: /raid/sup_data_path_44100_new\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: null\n",
      "      max_duration: 20\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 73.4161911010742\n",
      "      pitch_fmax: 2093.004638671875\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 183.15986328125\n",
      "      pitch_std: 81.13707580566407\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 24\n",
      "      num_workers: 1\n",
      "    \n",
      "[NeMo W 2025-12-03 21:29:33 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /ws/HiFiTTS/manifest_dev_ngc.json\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: /raid/sup_data_path_44100_new\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: null\n",
      "      max_duration: 20\n",
      "      min_duration: 0.2\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 73.4161911010742\n",
      "      pitch_fmax: 2093.004638671875\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 183.15986328125\n",
      "      pitch_std: 81.13707580566407\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 24\n",
      "      num_workers: 1\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-03 21:29:33 features:289] PADDING: 1\n",
      "[NeMo I 2025-12-03 21:29:34 save_restore_connector:249] Model FastPitchModel was successfully restored from /home/simon/fastpitch_10speaker.nemo.\n"
     ]
    }
   ],
   "source": [
    "model_path = Path(\"./fastpitch_10speaker.nemo\")\n",
    "pretrained_model = FastPitchModel.restore_from(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a4ec348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-03 21:29:34 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_1.23.0/tts_en_hifitts_hifigan_ft_fastpitch/dcbdefa79d5e3587c93636d4f53260b8/tts_en_hifitts_hifigan_ft_fastpitch.nemo.\n",
      "[NeMo I 2025-12-03 21:29:34 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.23.0/tts_en_hifitts_hifigan_ft_fastpitch/dcbdefa79d5e3587c93636d4f53260b8/tts_en_hifitts_hifigan_ft_fastpitch.nemo\n",
      "[NeMo I 2025-12-03 21:29:34 common:924] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-12-03 21:29:34 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.VocoderDataset\n",
      "      manifest_filepath: /manifests/hifitts_fp_1000epoch_mels_train.json\n",
      "      sample_rate: 44100\n",
      "      n_segments: 16384\n",
      "      max_duration: null\n",
      "      min_duration: 0.75\n",
      "      load_precomputed_mel: true\n",
      "      hop_length: 512\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 100\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2025-12-03 21:29:34 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.VocoderDataset\n",
      "      manifest_filepath: /manifests/hifitts_fp_1000epoch_mels_test.json\n",
      "      sample_rate: 44100\n",
      "      n_segments: 131072\n",
      "      max_duration: null\n",
      "      min_duration: 3\n",
      "      load_precomputed_mel: true\n",
      "      hop_length: 512\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 16\n",
      "      num_workers: 4\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-03 21:29:34 features:289] PADDING: 0\n",
      "[NeMo I 2025-12-03 21:29:34 features:297] STFT using exact pad\n",
      "[NeMo I 2025-12-03 21:29:34 features:289] PADDING: 0\n",
      "[NeMo I 2025-12-03 21:29:34 features:297] STFT using exact pad\n",
      "[NeMo I 2025-12-03 21:29:35 save_restore_connector:249] Model HifiGanModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/tts_en_hifitts_hifigan_ft_fastpitch/dcbdefa79d5e3587c93636d4f53260b8/tts_en_hifitts_hifigan_ft_fastpitch.nemo.\n"
     ]
    }
   ],
   "source": [
    "vocoder = HifiGanModel.from_pretrained(\"tts_en_hifitts_hifigan_ft_fastpitch\")\n",
    "vocoder = vocoder.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92848ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def infer_multispeaker(spec_gen_model, vocoder_model, text, speaker_id=0, pace=1.0):\n",
    "    \n",
    "    spec_gen_model.eval()\n",
    "    vocoder_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      \n",
    "        tokens = spec_gen_model.parse(text)\n",
    "        if tokens.ndim == 1:\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "\n",
    "        device = next(spec_gen_model.parameters()).device\n",
    "        tokens = tokens.to(device)\n",
    "\n",
    "      \n",
    "        n_speakers = getattr(spec_gen_model, \"n_speakers\", 1) \n",
    "        if n_speakers > 1:\n",
    "            if speaker_id is None or speaker_id >= n_speakers:\n",
    "                raise ValueError(f\"speaker_id must be < {n_speakers}, got {speaker_id}\")\n",
    "            speaker = torch.tensor([speaker_id], device=device, dtype=torch.long)\n",
    "        else:\n",
    "            speaker = None\n",
    "            \n",
    "        spectrogram = spec_gen_model.generate_spectrogram(\n",
    "            tokens=tokens,\n",
    "            speaker=speaker,\n",
    "            pace=pace,\n",
    "        )\n",
    "\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "\n",
    "    if isinstance(spectrogram, torch.Tensor):\n",
    "        spectrogram = spectrogram.squeeze().cpu().numpy()\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.squeeze().cpu().numpy()\n",
    "\n",
    "    return spectrogram, audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45c2b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_last_ckpt(exp_dir, model_name):\n",
    " \n",
    "    base = Path(exp_dir) / model_name\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(f\"Nincs ilyen mappa: {base}\")\n",
    "\n",
    "    runs = sorted(\n",
    "    [d for d in base.iterdir() if d.is_dir() and \".ipynb_checkpoints\" not in str(d)],\n",
    "    key=lambda x: x.stat().st_mtime\n",
    ")\n",
    "\n",
    "    if not runs:\n",
    "        raise FileNotFoundError(f\"Nincs kísérlet a {base} mappában.\")\n",
    "\n",
    "    last_run = runs[-1]\n",
    "    ckpt_dir = last_run / \"checkpoints\"\n",
    "\n",
    "    last_ckpts = sorted(ckpt_dir.glob(\"*-last.ckpt\"))\n",
    "    if not last_ckpts:\n",
    "        raise FileNotFoundError(f\"Nincs 'last.ckpt' a {ckpt_dir} mappában.\")\n",
    "\n",
    "    ckpt_path = str(last_ckpts[-1])\n",
    "\n",
    "    return ckpt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44785c24",
   "metadata": {},
   "source": [
    "### modellek betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f296da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n",
      "I1203 21:29:38.569052 140008601651008 tokenize_and_classify.py:87] Creating ClassifyFst grammars.\n",
      "[NeMo W 2025-12-03 21:30:05 en_us_arpabet:66] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
      "[NeMo W 2025-12-03 21:30:05 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.dataset.TTSDataset\n",
      "      manifest_filepath: /home/simon/ESD/Neutral_train.json\n",
      "      n_speakers: 10\n",
      "      speaker_column: speaker\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: ./MultiSpeaker/NeMoTTS_sup_data\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65\n",
      "      pitch_fmax: 2045\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 168.0\n",
      "      pitch_std: 69.0\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 20\n",
      "      num_workers: 12\n",
      "      pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-12-03 21:30:05 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.dataset.TTSDataset\n",
      "      manifest_filepath: /home/simon/ESD/Neutral_dev.json\n",
      "      n_speakers: 10\n",
      "      speaker_column: speaker\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: ./MultiSpeaker/NeMoTTS_sup_data\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: null\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65\n",
      "      pitch_fmax: 2045\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 168.0\n",
      "      pitch_std: 69.0\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 20\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-03 21:30:05 features:289] PADDING: 1\n",
      "Új speaker embedding inicializálása (10 beszélő)...\n"
     ]
    }
   ],
   "source": [
    "exp_dir = \"./multi_neutral_speech\"\n",
    "\n",
    "last_ckpt_neutral = get_last_ckpt(exp_dir, \"FastPitch\")\n",
    "checkpoint_neutral = torch.load(last_ckpt_neutral, weights_only=False) \n",
    "\n",
    "spec_model_neutral = FastPitchModel.load_from_checkpoint(last_ckpt_neutral, map_location=\"cuda\")\n",
    "spec_model_neutral.eval().cuda()\n",
    "\n",
    "if not hasattr(spec_model_neutral, \"speaker_emb\") or spec_model_neutral.speaker_emb is None:\n",
    "    print(\"Új speaker embedding inicializálása (10 beszélő)...\")\n",
    "    spec_model_neutral.speaker_emb = torch.nn.Embedding(10, 384).cuda()\n",
    "    spec_model_neutral.n_speakers = 10\n",
    "elif spec_model_neutral.n_speakers < 10:\n",
    "    print(\"Frissítjük n_speakers értékét → 10\")\n",
    "    spec_model_neutral.n_speakers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b556766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ellenőrzés: 10 torch.Size([10, 384])\n"
     ]
    }
   ],
   "source": [
    "print(\"Ellenőrzés:\", spec_model_neutral.n_speakers, spec_model_neutral.speaker_emb.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d69525ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n",
      "I1203 21:30:08.791059 140008601651008 tokenize_and_classify.py:87] Creating ClassifyFst grammars.\n",
      "[NeMo W 2025-12-03 21:30:37 en_us_arpabet:66] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
      "[NeMo W 2025-12-03 21:30:37 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.dataset.TTSDataset\n",
      "      manifest_filepath: /home/simon/ESD/Sad_train.json\n",
      "      n_speakers: 10\n",
      "      speaker_column: speaker\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: ./MultiSpeaker/NeMoTTS_sup_data_sad\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65\n",
      "      pitch_fmax: 1897\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 177.0\n",
      "      pitch_std: 70.0\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 20\n",
      "      num_workers: 12\n",
      "      pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-12-03 21:30:37 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.dataset.TTSDataset\n",
      "      manifest_filepath: /home/simon/ESD/Sad_dev.json\n",
      "      n_speakers: 10\n",
      "      speaker_column: speaker\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: ./MultiSpeaker/NeMoTTS_sup_data_sad\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: null\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65\n",
      "      pitch_fmax: 1897\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 177.0\n",
      "      pitch_std: 70.0\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 20\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-03 21:30:37 features:289] PADDING: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FastPitchModel(\n",
       "  (mel_loss_fn): MelLoss()\n",
       "  (pitch_loss_fn): PitchLoss()\n",
       "  (duration_loss_fn): DurationLoss()\n",
       "  (energy_loss_fn): EnergyLoss()\n",
       "  (aligner): AlignmentEncoder(\n",
       "    (cond_input): ConditionalInput()\n",
       "    (softmax): Softmax(dim=3)\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (key_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (query_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): ReLU()\n",
       "      (4): ConvNorm(\n",
       "        (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (forward_sum_loss_fn): ForwardSumLoss(\n",
       "    (log_softmax): LogSoftmax(dim=-1)\n",
       "    (ctc_loss): CTCLoss()\n",
       "  )\n",
       "  (bin_loss_fn): BinLoss()\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (fastpitch): FastPitchModule(\n",
       "    (encoder): FFTransformerEncoder(\n",
       "      (pos_emb): PositionalEmbedding()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (cond_input): ConditionalInput()\n",
       "      (word_emb): Embedding(115, 384, padding_idx=112)\n",
       "    )\n",
       "    (decoder): FFTransformerDecoder(\n",
       "      (pos_emb): PositionalEmbedding()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (cond_input): ConditionalInput()\n",
       "    )\n",
       "    (duration_predictor): TemporalPredictor(\n",
       "      (cond_input): ConditionalInput()\n",
       "      (layers): ModuleList(\n",
       "        (0): ConvReLUNorm(\n",
       "          (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): ConvReLUNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (pitch_predictor): TemporalPredictor(\n",
       "      (cond_input): ConditionalInput()\n",
       "      (layers): ModuleList(\n",
       "        (0): ConvReLUNorm(\n",
       "          (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): ConvReLUNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (aligner): AlignmentEncoder(\n",
       "      (cond_input): ConditionalInput()\n",
       "      (softmax): Softmax(dim=3)\n",
       "      (log_softmax): LogSoftmax(dim=3)\n",
       "      (key_proj): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): ConvNorm(\n",
       "          (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (query_proj): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): ConvNorm(\n",
       "          (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (3): ReLU()\n",
       "        (4): ConvNorm(\n",
       "          (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (speaker_emb): Embedding(10, 384)\n",
       "    (pitch_emb): Conv1d(1, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (proj): Linear(in_features=384, out_features=80, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"./multi_sad_speech\"\n",
    "\n",
    "last_ckpt_sad = get_last_ckpt(exp_dir, \"FastPitch\")\n",
    "checkpoint_sad = torch.load(last_ckpt_sad, weights_only=False) \n",
    "\n",
    "spec_model_sad = FastPitchModel.load_from_checkpoint(last_ckpt_sad, map_location=\"cuda\")\n",
    "spec_model_sad.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3479a65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Új speaker embedding inicializálása (10 beszélő)...\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(spec_model_sad, \"speaker_emb\") or spec_model_sad.speaker_emb is None:\n",
    "    print(\"Új speaker embedding inicializálása (10 beszélő)...\")\n",
    "    spec_model_sad.speaker_emb = torch.nn.Embedding(10, 384).cuda()\n",
    "    spec_model_sad.n_speakers = 10\n",
    "elif spec_model_sad.n_speakers < 10:\n",
    "    print(\"Frissítjük n_speakers értékét → 10\")\n",
    "    spec_model_sad.n_speakers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d1f0f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n",
      "I1203 21:30:41.288490 140008601651008 tokenize_and_classify.py:87] Creating ClassifyFst grammars.\n",
      "[NeMo W 2025-12-03 21:31:12 en_us_arpabet:66] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
      "[NeMo W 2025-12-03 21:31:12 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.dataset.TTSDataset\n",
      "      manifest_filepath: /home/simon/ESD/Happy_train.json\n",
      "      n_speakers: 10\n",
      "      speaker_column: speaker\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: ./MultiSpeaker/NeMoTTS_sup_data_happy\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65\n",
      "      pitch_fmax: 2045\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 239.0\n",
      "      pitch_std: 131.0\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 20\n",
      "      num_workers: 12\n",
      "      pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-12-03 21:31:12 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.dataset.TTSDataset\n",
      "      manifest_filepath: /home/simon/ESD/Happy_dev.json\n",
      "      n_speakers: 10\n",
      "      speaker_column: speaker\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: ./MultiSpeaker/NeMoTTS_sup_data_happy\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: null\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65\n",
      "      pitch_fmax: 2045\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 239.0\n",
      "      pitch_std: 131.0\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 20\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-03 21:31:12 features:289] PADDING: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FastPitchModel(\n",
       "  (mel_loss_fn): MelLoss()\n",
       "  (pitch_loss_fn): PitchLoss()\n",
       "  (duration_loss_fn): DurationLoss()\n",
       "  (energy_loss_fn): EnergyLoss()\n",
       "  (aligner): AlignmentEncoder(\n",
       "    (cond_input): ConditionalInput()\n",
       "    (softmax): Softmax(dim=3)\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (key_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (query_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): ReLU()\n",
       "      (4): ConvNorm(\n",
       "        (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (forward_sum_loss_fn): ForwardSumLoss(\n",
       "    (log_softmax): LogSoftmax(dim=-1)\n",
       "    (ctc_loss): CTCLoss()\n",
       "  )\n",
       "  (bin_loss_fn): BinLoss()\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (fastpitch): FastPitchModule(\n",
       "    (encoder): FFTransformerEncoder(\n",
       "      (pos_emb): PositionalEmbedding()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (cond_input): ConditionalInput()\n",
       "      (word_emb): Embedding(115, 384, padding_idx=112)\n",
       "    )\n",
       "    (decoder): FFTransformerDecoder(\n",
       "      (pos_emb): PositionalEmbedding()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (cond_input): ConditionalInput()\n",
       "    )\n",
       "    (duration_predictor): TemporalPredictor(\n",
       "      (cond_input): ConditionalInput()\n",
       "      (layers): ModuleList(\n",
       "        (0): ConvReLUNorm(\n",
       "          (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): ConvReLUNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (pitch_predictor): TemporalPredictor(\n",
       "      (cond_input): ConditionalInput()\n",
       "      (layers): ModuleList(\n",
       "        (0): ConvReLUNorm(\n",
       "          (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): ConvReLUNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (aligner): AlignmentEncoder(\n",
       "      (cond_input): ConditionalInput()\n",
       "      (softmax): Softmax(dim=3)\n",
       "      (log_softmax): LogSoftmax(dim=3)\n",
       "      (key_proj): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): ConvNorm(\n",
       "          (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (query_proj): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): ConvNorm(\n",
       "          (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (3): ReLU()\n",
       "        (4): ConvNorm(\n",
       "          (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (speaker_emb): Embedding(10, 384)\n",
       "    (pitch_emb): Conv1d(1, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (proj): Linear(in_features=384, out_features=80, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"./multi_happy_speech\"\n",
    "\n",
    "last_ckpt_happy = get_last_ckpt(exp_dir, \"FastPitch\")\n",
    "checkpoint_happy = torch.load(last_ckpt_happy, weights_only=False)  \n",
    "\n",
    "spec_model_happy = FastPitchModel.load_from_checkpoint(last_ckpt_happy, map_location=\"cuda\")\n",
    "spec_model_happy.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c36493e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Új speaker embedding inicializálása (10 beszélő)...\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(spec_model_happy, \"speaker_emb\") or spec_model_happy.speaker_emb is None:\n",
    "    print(\"Új speaker embedding inicializálása (10 beszélő)...\")\n",
    "    spec_model_happy.speaker_emb = torch.nn.Embedding(10, 384).cuda()\n",
    "    spec_model_happy.n_speakers = 10\n",
    "elif spec_model_happy.n_speakers < 10:\n",
    "    print(\"Frissítjük n_speakers értékét → 10\")\n",
    "    spec_model_happy.n_speakers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d669aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n",
      "I1203 21:31:16.376009 140008601651008 tokenize_and_classify.py:87] Creating ClassifyFst grammars.\n",
      "[NeMo W 2025-12-03 21:31:48 en_us_arpabet:66] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
      "[NeMo W 2025-12-03 21:31:48 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.dataset.TTSDataset\n",
      "      manifest_filepath: /home/simon/ESD/Angry_train.json\n",
      "      n_speakers: 10\n",
      "      speaker_column: speaker\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: ./MultiSpeaker/NeMoTTS_sup_data_angry\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65\n",
      "      pitch_fmax: 2045\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 208\n",
      "      pitch_std: 104\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 20\n",
      "      num_workers: 12\n",
      "      pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-12-03 21:31:48 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.dataset.TTSDataset\n",
      "      manifest_filepath: /home/simon/ESD/Angry_dev.json\n",
      "      n_speakers: 10\n",
      "      speaker_column: speaker\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: ./MultiSpeaker/NeMoTTS_sup_data_angry\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: null\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65\n",
      "      pitch_fmax: 2045\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 208\n",
      "      pitch_std: 104\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 20\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-03 21:31:48 features:289] PADDING: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FastPitchModel(\n",
       "  (mel_loss_fn): MelLoss()\n",
       "  (pitch_loss_fn): PitchLoss()\n",
       "  (duration_loss_fn): DurationLoss()\n",
       "  (energy_loss_fn): EnergyLoss()\n",
       "  (aligner): AlignmentEncoder(\n",
       "    (cond_input): ConditionalInput()\n",
       "    (softmax): Softmax(dim=3)\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (key_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (query_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): ReLU()\n",
       "      (4): ConvNorm(\n",
       "        (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (forward_sum_loss_fn): ForwardSumLoss(\n",
       "    (log_softmax): LogSoftmax(dim=-1)\n",
       "    (ctc_loss): CTCLoss()\n",
       "  )\n",
       "  (bin_loss_fn): BinLoss()\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (fastpitch): FastPitchModule(\n",
       "    (encoder): FFTransformerEncoder(\n",
       "      (pos_emb): PositionalEmbedding()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (cond_input): ConditionalInput()\n",
       "      (word_emb): Embedding(115, 384, padding_idx=112)\n",
       "    )\n",
       "    (decoder): FFTransformerDecoder(\n",
       "      (pos_emb): PositionalEmbedding()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (cond_input): ConditionalInput()\n",
       "    )\n",
       "    (duration_predictor): TemporalPredictor(\n",
       "      (cond_input): ConditionalInput()\n",
       "      (layers): ModuleList(\n",
       "        (0): ConvReLUNorm(\n",
       "          (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): ConvReLUNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (pitch_predictor): TemporalPredictor(\n",
       "      (cond_input): ConditionalInput()\n",
       "      (layers): ModuleList(\n",
       "        (0): ConvReLUNorm(\n",
       "          (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): ConvReLUNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (aligner): AlignmentEncoder(\n",
       "      (cond_input): ConditionalInput()\n",
       "      (softmax): Softmax(dim=3)\n",
       "      (log_softmax): LogSoftmax(dim=3)\n",
       "      (key_proj): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): ConvNorm(\n",
       "          (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (query_proj): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): ConvNorm(\n",
       "          (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (3): ReLU()\n",
       "        (4): ConvNorm(\n",
       "          (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (speaker_emb): Embedding(10, 384)\n",
       "    (pitch_emb): Conv1d(1, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (proj): Linear(in_features=384, out_features=80, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"./multi_angry_speech\"\n",
    "\n",
    "last_ckpt_angry = get_last_ckpt(exp_dir, \"FastPitch\")\n",
    "checkpoint_angry = torch.load(last_ckpt_angry, weights_only=False) \n",
    "\n",
    "spec_model_angry = FastPitchModel.load_from_checkpoint(last_ckpt_angry, map_location=\"cuda\")\n",
    "spec_model_angry.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "137e9076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Új speaker embedding inicializálása (10 beszélő)...\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(spec_model_angry, \"speaker_emb\") or spec_model_angry.speaker_emb is None:\n",
    "    print(\"Új speaker embedding inicializálása (10 beszélő)...\")\n",
    "    spec_model_angry.speaker_emb = torch.nn.Embedding(10, 384).cuda()\n",
    "    spec_model_angry.n_speakers = 10\n",
    "elif spec_model_angry.n_speakers < 10:\n",
    "    print(\"Frissítjük n_speakers értékét → 10\")\n",
    "    spec_model_angry.n_speakers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e37082cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n",
      "I1203 21:31:52.342900 140008601651008 tokenize_and_classify.py:87] Creating ClassifyFst grammars.\n",
      "[NeMo W 2025-12-03 21:32:27 en_us_arpabet:66] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
      "[NeMo W 2025-12-03 21:32:27 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.dataset.TTSDataset\n",
      "      manifest_filepath: /home/simon/ESD/Surprise_train.json\n",
      "      n_speakers: 10\n",
      "      speaker_column: speaker\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: ./MultiSpeaker/NeMoTTS_sup_data_surprise\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65\n",
      "      pitch_fmax: 2045\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 271.0\n",
      "      pitch_std: 173.0\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 20\n",
      "      num_workers: 12\n",
      "      pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-12-03 21:32:27 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.dataset.TTSDataset\n",
      "      manifest_filepath: /home/simon/ESD/Surprise_dev.json\n",
      "      n_speakers: 10\n",
      "      speaker_column: speaker\n",
      "      sample_rate: 44100\n",
      "      sup_data_path: ./MultiSpeaker/NeMoTTS_sup_data_surprise\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      - speaker_id\n",
      "      n_fft: 2048\n",
      "      win_length: 2048\n",
      "      hop_length: 512\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: null\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65\n",
      "      pitch_fmax: 2045\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 271.0\n",
      "      pitch_std: 173.0\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 20\n",
      "      num_workers: 8\n",
      "      pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-03 21:32:27 features:289] PADDING: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FastPitchModel(\n",
       "  (mel_loss_fn): MelLoss()\n",
       "  (pitch_loss_fn): PitchLoss()\n",
       "  (duration_loss_fn): DurationLoss()\n",
       "  (energy_loss_fn): EnergyLoss()\n",
       "  (aligner): AlignmentEncoder(\n",
       "    (cond_input): ConditionalInput()\n",
       "    (softmax): Softmax(dim=3)\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (key_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (query_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): ReLU()\n",
       "      (4): ConvNorm(\n",
       "        (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (forward_sum_loss_fn): ForwardSumLoss(\n",
       "    (log_softmax): LogSoftmax(dim=-1)\n",
       "    (ctc_loss): CTCLoss()\n",
       "  )\n",
       "  (bin_loss_fn): BinLoss()\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (fastpitch): FastPitchModule(\n",
       "    (encoder): FFTransformerEncoder(\n",
       "      (pos_emb): PositionalEmbedding()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (cond_input): ConditionalInput()\n",
       "      (word_emb): Embedding(115, 384, padding_idx=112)\n",
       "    )\n",
       "    (decoder): FFTransformerDecoder(\n",
       "      (pos_emb): PositionalEmbedding()\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerLayer(\n",
       "          (dec_attn): MultiHeadAttn(\n",
       "            (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "            (dropatt): Dropout(p=0.1, inplace=False)\n",
       "            (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (pos_ff): PositionwiseConvFF(\n",
       "            (CoreNet): Sequential(\n",
       "              (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): ConditionalLayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (cond_input): ConditionalInput()\n",
       "    )\n",
       "    (duration_predictor): TemporalPredictor(\n",
       "      (cond_input): ConditionalInput()\n",
       "      (layers): ModuleList(\n",
       "        (0): ConvReLUNorm(\n",
       "          (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): ConvReLUNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (pitch_predictor): TemporalPredictor(\n",
       "      (cond_input): ConditionalInput()\n",
       "      (layers): ModuleList(\n",
       "        (0): ConvReLUNorm(\n",
       "          (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): ConvReLUNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): ConditionalLayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (aligner): AlignmentEncoder(\n",
       "      (cond_input): ConditionalInput()\n",
       "      (softmax): Softmax(dim=3)\n",
       "      (log_softmax): LogSoftmax(dim=3)\n",
       "      (key_proj): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): ConvNorm(\n",
       "          (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (query_proj): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): ConvNorm(\n",
       "          (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (3): ReLU()\n",
       "        (4): ConvNorm(\n",
       "          (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (speaker_emb): Embedding(10, 384)\n",
       "    (pitch_emb): Conv1d(1, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (proj): Linear(in_features=384, out_features=80, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"./multi_surprise_speech\"\n",
    "\n",
    "last_ckpt_surprise = get_last_ckpt(exp_dir, \"FastPitch\")\n",
    "checkpoint_surprise = torch.load(last_ckpt_surprise, weights_only=False)\n",
    "\n",
    "spec_model_surprise = FastPitchModel.load_from_checkpoint(last_ckpt_surprise, map_location=\"cuda\")\n",
    "spec_model_surprise.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9486ecbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Új speaker embedding inicializálása (10 beszélő)...\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(spec_model_surprise, \"speaker_emb\") or spec_model_surprise.speaker_emb is None:\n",
    "    print(\"Új speaker embedding inicializálása (10 beszélő)...\")\n",
    "    spec_model_surprise.speaker_emb = torch.nn.Embedding(10, 384).cuda()\n",
    "    spec_model_surprise.n_speakers = 10\n",
    "elif spec_model_surprise.n_speakers < 10:\n",
    "    print(\"Frissítjük n_speakers értékét → 10\")\n",
    "    spec_model_surprise.n_speakers = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ddb38",
   "metadata": {},
   "source": [
    "## gpt felolvastatás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beda1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6e2c35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunkok száma: 248\n",
      "Embedding és index létrehozva.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_data = load_pdf(\"./GPT/Everything_about_VIK.pdf\")\n",
    "\n",
    "chunks = smart_chunk_text(text_data)\n",
    "print(\"Chunkok száma:\", len(chunks))\n",
    "\n",
    "index, embeddings, text_data = create_embeddings(chunks)\n",
    "print(\"Embedding és index létrehozva.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "063dfc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = [\n",
    "    \"Default\",\n",
    "    \"1 - Male\",\n",
    "    \"2 - Male\",\n",
    "    \"3 - Male\",\n",
    "    \"4 - Male\",\n",
    "    \"5 - Female\",\n",
    "    \"6 - Female\",\n",
    "    \"7 - Female\",\n",
    "    \"8 - Female\",\n",
    "    \"9 - Female\",\n",
    "    \"10 - Male\"\n",
    "]\n",
    "messages=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d9742d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def estimate_speech_speed(audio_path, text):\n",
    "   \n",
    "    duration_sec = librosa.get_duration(path=audio_path)\n",
    "    duration_min = duration_sec / 60\n",
    "    \n",
    "    words = text.strip().split()\n",
    "    num_words = len(words)\n",
    "\n",
    "    if duration_min > 0:\n",
    "        wpm = num_words / duration_min\n",
    "    else:\n",
    "        wpm = 0\n",
    "\n",
    "    if wpm < 50:\n",
    "        speed_label = \"slow\"\n",
    "    elif wpm < 140:\n",
    "        speed_label = \"normal\"\n",
    "    else:\n",
    "        speed_label = \"fast\"\n",
    "\n",
    "    return wpm, speed_label, num_words, duration_sec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "42f47aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_and_tts(user_input, speaker_choice,pred_emo,gender,speed_label):\n",
    "    global messages\n",
    "    \n",
    "    print(f\"Input: '{user_input}'\")  \n",
    "    print(f\"Speaker: {speaker_choice}, Emotion: {pred_emo}, Gender: {gender}\")  \n",
    "    \n",
    "    if not user_input.strip():\n",
    "        return \"Please enter a question.\", None, \"neutral\"\n",
    "   \n",
    "    context = retrieve_context(user_input, index, text_data, top_k=8)\n",
    "    system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\":\n",
    "        \"You are an empathetic, emotionally adaptive AI assistant designed specifically for helping first-year \"\n",
    "        \"students at the Budapest University of Technology and Economics (BME). Always respond in English. \"\n",
    "        \"You should assume that every question relates to university life, BME studies, courses, schedules, \"\n",
    "        \"student responsibilities, or general academic difficulties—unless the user clearly indicates otherwise. \"\n",
    "        \"Your primary goal is to support the student emotionally while also giving accurate, context-based answers. \"\n",
    "        \"Always check the provided RAG context first. If the context contains information relevant to the question, \"\n",
    "        \"you MUST rely only on that information. If the context does NOT contain relevant information, give a short, \"\n",
    "        \"general, helpful answer, but NEVER hallucinate facts about BME, courses, requirements, deadlines, or policies. \"\n",
    "        \"If something is unknown, say it kindly and offer general academic guidance instead. \"\n",
    "        \"Your answers must be concise: maximum 3 short, clear sentences. \"\n",
    "        \"Never mention the context directly. Never explain your reasoning process. \"\n",
    "        \"Emotion handling rules: \"\n",
    "        \"- Neutral user: stay calm, slightly positive, and supportive. \"\n",
    "        \"- Happy user: lightly mirror their positivity. \"\n",
    "        \"- Sad user: be warm and empathetic but not overly emotional. \"\n",
    "        \"- Surprised user: respond calmly and confidently. \"\n",
    "        \"- Angry user: do NOT mirror anger; stay steady, understanding, and solution-oriented. \"\n",
    "        \"Audio-based emotion predictions are uncertain and should only influence your tone softly. \"\n",
    "        \"Output format requirement: always respond strictly as a JSON object: \"\n",
    "        \"{\\\"text\\\": <your answer>, \\\"emotion\\\": <one of [happy, sad, angry, neutral, surprise]>}. \"\n",
    "        \"Do not include anything outside this JSON. \"\n",
    "        f\"RAG context (may or may not contain relevant information): {context}\\n\"\n",
    "        f\"Predicted user emotion from audio (uncertain): {pred_emo}\"\n",
    "    }\n",
    "\n",
    "    \n",
    "    conversation_input = [\n",
    "        system_prompt,\n",
    "        *messages,\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "\n",
    "    print(datetime.now())\n",
    "   \n",
    "    chat = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=conversation_input\n",
    "    )\n",
    "    \n",
    "    reply = chat.choices[0].message.content.strip()\n",
    "    print(datetime.now())\n",
    "    print(f\"GPT válasz: '{reply}'\")  \n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "   \n",
    "    MAX_PAIRS = 15\n",
    "\n",
    "    if len(messages) > MAX_PAIRS * 2:\n",
    "        messages = messages[-MAX_PAIRS*2:]\n",
    "\n",
    "    \n",
    "    if speaker_choice==\"Default\":\n",
    "        if gender==\"Male\":\n",
    "            speaker_id=6\n",
    "        else:\n",
    "            speaker_id=1\n",
    "    else:    \n",
    "        speaker_id = int(speaker_choice.split(\" \")[0]) - 1\n",
    "\n",
    "        \n",
    "    print(f\"Speaker ID: {speaker_id}\")  \n",
    "\n",
    "    try:\n",
    "        data = json.loads(reply)\n",
    "        text = data[\"text\"]\n",
    "        \n",
    "        emotion = data[\"emotion\"].lower()\n",
    "    except:\n",
    "        print(f\"JSON hiba: {e}\")  \n",
    "        return \"JSON parse error\", None, \"neutral\"\n",
    "\n",
    "    model_map = {\n",
    "        \"happy\": spec_model_happy,\n",
    "        \"sad\": spec_model_sad,\n",
    "        \"angry\": spec_model_angry,\n",
    "        \"neutral\": spec_model_neutral,\n",
    "        \"surprise\": spec_model_surprise\n",
    "    }\n",
    "    \n",
    "    if speed_label == \"slow\":\n",
    "        speed = 0.8\n",
    "    elif speed_label == \"fast\":\n",
    "        speed = 1.2\n",
    "    else:\n",
    "        speed = 1.0\n",
    "\n",
    "\n",
    "    selected = model_map.get(emotion, spec_model_neutral)\n",
    "\n",
    "    \n",
    "    \n",
    "    spec, audio = infer_multispeaker(\n",
    "        spec_gen_model=selected,\n",
    "        vocoder_model=vocoder,\n",
    "        text=text,\n",
    "        speaker_id=speaker_id,\n",
    "        pace=speed\n",
    "    )\n",
    "\n",
    "\n",
    "    audio_np = np.array(audio, dtype=np.float32)\n",
    "\n",
    "    output_path=f\"output_{int(time.time() * 1000)}.wav\"\n",
    "    print(datetime.now())\n",
    "    sf.write(output_path, audio_np.T, 44100)\n",
    "    print(f\" Hang mentve: {output_path}\") \n",
    "\n",
    "    \n",
    "    return text, output_path, emotion \n",
    "\n",
    "def chat_and_tts_from_speech(audio_input, speaker_choice):\n",
    "    if audio_input is None:\n",
    "        return \"No audio detected.\", None, \"neutral\"\n",
    "\n",
    "    \n",
    "    samplerate, data = audio_input\n",
    "    wav_path = \"record.wav\"\n",
    "    write(wav_path, samplerate, data.astype(np.int16))\n",
    "\n",
    "\n",
    "    result = whisper(wav_path)\n",
    "    recognized_text = result[\"text\"]\n",
    "    \n",
    "    print(f\" Whisper felismerte: '{recognized_text}'\")\n",
    "    \n",
    "    noise_words = {\n",
    "        \"you\", \"yo\", \"yeah\", \"ya\", \"okay\", \"ok\", \"okey\", \"alright\",\n",
    "        \"thank you\", \"thanks\",\"ha\", \"ah\", \"uh\", \"um\", \"hmm\", \"mmm\", \"uhh\", \"eh\", \"oh\", \"huh\",\n",
    "        \"mmmhm\", \"mmhmm\", \"mhmm\",\"m\", \"mm\", \"mmm\", \"h\", \"uhm\", \"erm\",\"hey\", \"hi\", \"yo.\", \"huh.\", \"oh.\", \"so\", \"well\", \"right\",\n",
    "        \"oh okay\", \"oh ok\", \"i see\", \"hmm okay\",\"\", \" \", \".\", \",\", \"?\", \"!\", \"-\", \"_\"\n",
    "    }\n",
    "    normalized_text = recognized_text.lower().strip()\n",
    "\n",
    "    \n",
    "    normalized_text = normalized_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    normalized_text = \" \".join(normalized_text.split())\n",
    "\n",
    "\n",
    "\n",
    "    if normalized_text in noise_words:\n",
    "        print(\"Noise word detected → ignored.\")\n",
    "        return \"\", None, \"neutral\", recognized_text\n",
    "    \n",
    "    wpm, speed_label, num_words, duration_sec = estimate_speech_speed(wav_path, recognized_text)\n",
    "    print(f\" Beszédtempó: {wpm:.1f} WPM ({speed_label}) | {num_words} szó, {duration_sec:.2f} mp\")\n",
    "    print(datetime.now())\n",
    "    \n",
    "    audio_path=wav_path\n",
    "    predicted_emotion = predict_emotion(audio_path, model, enc)\n",
    "    print(f\" Érzelem: {predicted_emotion}\")\n",
    "    print(datetime.now())\n",
    "    \n",
    "   \n",
    "    gender = estimate_gender(audio_path)\n",
    "\n",
    "    print(f\"Nem: {gender}\")\n",
    "    print(datetime.now())\n",
    "    text_reply, audio_output_path, emotion = chat_and_tts(recognized_text, speaker_choice,predicted_emotion,gender,speed_label)\n",
    "    print(f\"Válasz: '{text_reply}'\")  \n",
    "    print(f\"Hangfájl: {audio_output_path}\")  \n",
    "    print(f\"Kimenet érzelem: {emotion}\")  \n",
    "    \n",
    "    return text_reply, audio_output_path, emotion, recognized_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc5945b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4bc2d2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:8638\n",
      "* Running on public URL: https://2f10a467a527bba33c.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2f10a467a527bba33c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "audio_buffer = np.array([], dtype=np.float32)\n",
    "last_speech_time = 0\n",
    "is_recording = False\n",
    "\n",
    "MIN_SPEECH_DURATION = 1.0\n",
    "SILENCE_THRESHOLD = 0.030\n",
    "SILENCE_DURATION = 1.5\n",
    "MIN_CHUNK_LENGTH = 2000\n",
    "MIN_RMS_FOR_SPEECH = 0.028\n",
    "MIN_ZCR_FOR_SPEECH = 0.009\n",
    "MIN_CONTINUOUS_SPEECH = 0.15\n",
    "\n",
    "\n",
    "task_queue = queue.Queue()\n",
    "audio_results = queue.Queue()\n",
    "\n",
    "def background_processor():\n",
    "    while True:\n",
    "        sample_rate, audio_int16, speaker_choice = task_queue.get()\n",
    "\n",
    "        try:\n",
    "            text_reply, audio_path, emotion, recognized_text = chat_and_tts_from_speech(\n",
    "                (sample_rate, audio_int16),\n",
    "                speaker_choice\n",
    "            )\n",
    "            if audio_path and os.path.exists(audio_path):\n",
    "                audio_data, sr = sf.read(audio_path, dtype=\"float32\")\n",
    "                if audio_data.ndim > 1:\n",
    "                    audio_data = np.mean(audio_data, axis=1)\n",
    "\n",
    "                audio_results.put((sr, audio_data))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Background error:\", e)\n",
    "\n",
    "        task_queue.task_done()\n",
    "\n",
    "threading.Thread(target=background_processor, daemon=True).start()\n",
    "\n",
    "\n",
    "def handle_stream(audio_chunk, speaker_choice):\n",
    "    global audio_buffer, last_speech_time, is_recording\n",
    "\n",
    "    if audio_chunk is None:\n",
    "        return gr.update()\n",
    "\n",
    "    try:\n",
    "        if isinstance(audio_chunk, tuple):\n",
    "            sample_rate, audio_data = audio_chunk\n",
    "        else:\n",
    "            audio_data = audio_chunk\n",
    "            sample_rate = SAMPLE_RATE\n",
    "\n",
    "        if audio_data.ndim > 1:\n",
    "            audio_data = np.mean(audio_data, axis=1)\n",
    "\n",
    "        audio_data = audio_data.astype(np.float32)\n",
    "        if np.max(np.abs(audio_data)) > 1:\n",
    "            audio_data /= 32768.0\n",
    "\n",
    "        energy = np.sqrt(np.mean(audio_data ** 2))\n",
    "        zcr = np.mean(np.abs(np.diff(np.sign(audio_data))))\n",
    "        chunk_dur = len(audio_data) / sample_rate\n",
    "        valid = len(audio_data) >= MIN_CHUNK_LENGTH\n",
    "\n",
    "        has_speech = (\n",
    "            energy > SILENCE_THRESHOLD\n",
    "            and energy > MIN_RMS_FOR_SPEECH\n",
    "            and zcr > MIN_ZCR_FOR_SPEECH\n",
    "            and valid\n",
    "            and chunk_dur >= MIN_CONTINUOUS_SPEECH\n",
    "        )\n",
    "\n",
    "        now = time.time()\n",
    "\n",
    "        if has_speech and not is_recording:\n",
    "            is_recording = True\n",
    "            audio_buffer = audio_data.copy()\n",
    "            last_speech_time = now\n",
    "            return gr.update()\n",
    "\n",
    "        if is_recording:\n",
    "            audio_buffer = np.concatenate([audio_buffer, audio_data])\n",
    "            if has_speech:\n",
    "                last_speech_time = now\n",
    "\n",
    "        if is_recording:\n",
    "            silence = now - last_speech_time\n",
    "            duration = len(audio_buffer) / sample_rate\n",
    "\n",
    "            if silence >= SILENCE_DURATION and duration >= MIN_SPEECH_DURATION:\n",
    "                audio_int16 = (audio_buffer * 32767).astype(np.int16)\n",
    "                task_queue.put((sample_rate, audio_int16, speaker_choice))\n",
    "\n",
    "                audio_buffer = np.array([], dtype=np.float32)\n",
    "                is_recording = False\n",
    "                last_speech_time = 0\n",
    "\n",
    "                return gr.update()\n",
    "\n",
    "        return gr.update()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Stream error:\", e)\n",
    "        return gr.update()\n",
    "\n",
    "\n",
    "def poll_audio():\n",
    "    if audio_results.empty():\n",
    "        return gr.update()\n",
    "\n",
    "    sr, audio = audio_results.get()\n",
    "\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
    "    sf.write(tmp.name, audio, sr)\n",
    "\n",
    "    return tmp.name\n",
    "\n",
    "\n",
    "with gr.Blocks(\n",
    "    theme=\"soft\",\n",
    "    css=\"\"\"\n",
    "        /* Hide ONLY the output audio player UI, but keep it functional */\n",
    "        .output-audio-box audio {\n",
    "            display: none !important;\n",
    "        }\n",
    "        .output-audio-box {\n",
    "            height: 0px !important;\n",
    "            padding: 0 !important;\n",
    "            margin: 0 !important;\n",
    "            overflow: hidden !important;\n",
    "        }\n",
    "    \"\"\"\n",
    ") as demo:\n",
    "\n",
    "    gr.Markdown(\"## 🎙️ Emotional AI Assistant\")\n",
    "\n",
    "    with gr.Row():\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            audio_input = gr.Audio(type=\"numpy\", streaming=True, label=\"Microphone\")\n",
    "\n",
    "            speaker_drop = gr.Dropdown(\n",
    "                choices=speakers,\n",
    "                value=speakers[0],\n",
    "                label=\"Voice\"\n",
    "            )\n",
    "            voice_state = gr.State(speakers[0])\n",
    "\n",
    "            speaker_drop.change(lambda x: x, inputs=speaker_drop, outputs=voice_state)\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "           \n",
    "            audio_output = gr.Audio(\n",
    "                type=\"filepath\",\n",
    "                autoplay=True,\n",
    "                label=\"\",\n",
    "                elem_classes=[\"output-audio-box\"]\n",
    "            )\n",
    "\n",
    "    audio_input.stream(\n",
    "        fn=handle_stream,\n",
    "        inputs=[audio_input, voice_state],\n",
    "        outputs=[audio_output]\n",
    "    )\n",
    "\n",
    "    audio_input.change(\n",
    "        fn=poll_audio,\n",
    "        inputs=None,\n",
    "        outputs=audio_output\n",
    "    )\n",
    "\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=8638, share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88b5e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9060bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
